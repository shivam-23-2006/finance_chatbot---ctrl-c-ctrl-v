# app.py
import streamlit as st
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import os
from dotenv import load_dotenv

# --- Load environment variables ---
load_dotenv()
MODEL_NAME = os.getenv("MODEL_NAME")
HF_TOKEN = os.getenv("HF_TOKEN")

# --- Streamlit page config ---
st.set_page_config(
    page_title="Finance ChatBot",
    page_icon="ðŸ’°",
    layout="centered",
    initial_sidebar_state="expanded"
)

# --- Sidebar ---
st.sidebar.title("ðŸ’¡ Instructions")
st.sidebar.info(
    """
    - Ask finance-related questions in the input box below.
    - Responses are generated by **IBM Granite 2B** model.
    - CPU-only mode; large responses may take ~2-3 minutes.
    - Keep questions clear for best results.
    """
)
st.sidebar.title("ðŸ¤– Model Info")
st.sidebar.markdown(
    f"""
    - **Model:** {MODEL_NAME}  
    - **Parameters:** 2 Billion  
    - **Framework:** Hugging Face Transformers  
    - **Device:** CPU  
    """
)

# --- Welcome section ---
st.markdown(
    """
    <h1 style='text-align: center; color: #4B0082;'>ðŸ’° Welcome to the Finance ChatBot</h1>
    <p style='text-align: center; color: #555;'>Ask me anything about finance and I will try to help!</p>
    """,
    unsafe_allow_html=True
)

# --- IBM Granite model ---
@st.cache_resource
def load_model():
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_NAME,
        use_auth_token=HF_TOKEN,
        use_fast=False
    )
    
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        use_auth_token=HF_TOKEN,
        device_map="cpu"
    )
    
    return tokenizer, model

# Load model
tokenizer, model = load_model()

# --- Chat history session state ---
if "history" not in st.session_state:
    st.session_state.history = []

# --- User input area and Send button ---
user_input = st.text_area("Type your question here:", height=80)
send_pressed = st.button("Send")

if send_pressed and user_input.strip() != "":
    inputs = tokenizer(user_input, return_tensors="pt")
    
    # CPU-friendly inference
    with torch.inference_mode():
        outputs = model.generate(**inputs, max_new_tokens=150)
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # Add to chat history
    st.session_state.history.append({"user": user_input, "bot": response})
    
    # Clear input box after sending
    st.experimental_rerun()

# --- Scrollable chat history container ---
st.markdown("<h3 style='text-align:center;'>Chat History</h3>", unsafe_allow_html=True)
chat_container = st.container()
chat_html = "<div style='height:400px; overflow-y:scroll; padding:10px; border:1px solid #ccc; border-radius:10px;'>"

for chat in st.session_state.history:
    chat_html += f"""
    <div style='background-color:#DCF8C6; padding:10px; border-radius:10px; margin:10px 0;'>
        <b>You:</b> {chat['user']}
    </div>
    <div style='background-color:#EAEAEA; padding:10px; border-radius:10px; margin:10px 0;'>
        <b>Bot:</b> {chat['bot']}
    </div>
    """

chat_html += "</div>"
chat_container.markdown(chat_html, unsafe_allow_html=True)
